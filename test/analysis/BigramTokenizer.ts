import { BigramTokenizer } from '../../src/analysis/BigramTokenizer';
import { Token } from '../../src/analysis/Token';

/**
 * Utility function for BigramTokenizer
 * @param text String to tokenize by BigramTokenier
 * @returns Array of Token.termText
 */
function tokenize(text: string): string[] {
  const tokenizer = new BigramTokenizer();
  const tokens = tokenizer.tokenize(text);
  return tokens.map((token) => {
    expect(token).toBeInstanceOf(Token);
    return token.termText;
  });
}

describe('tokenizer', () => {
  test('should return empty array with empty string', () => {
    const tokens = tokenize('');
    expect(tokens).toEqual([]);
  });

  test('should return empty array with 1 letter', () => {
    const tokens = tokenize('a');
    expect(tokens).toEqual([]);
  });

  test('should return empty array with 1 letter of surrogate pair', () => {
    const tokens = tokenize('ğ©¸½');
    expect(tokens).toEqual([]);
  });

  test('should return empty array with 2 letters', () => {
    const tokens = tokenize('ab');
    expect(tokens).toEqual([ 'ab' ]);
  });

  test('should tokenize single-byte string into bi-grams', () => {
    const tokens = tokenize('abracadabra');
    expect(tokens).toEqual([ 'ab', 'br', 'ra', 'ac', 'ca', 'ad', 'da', 'ab', 'br', 'ra' ]);
  });

  test('should tokenize multi-byte string into bi-grams', () => {
    const tokens = tokenize('ã‚¢ãƒ–ãƒ©ã‚«ãƒ€ãƒ–ãƒ©');
    expect(tokens).toEqual([ 'ã‚¢ãƒ–', 'ãƒ–ãƒ©', 'ãƒ©ã‚«', 'ã‚«ãƒ€', 'ãƒ€ãƒ–', 'ãƒ–ãƒ©' ]);
  });

  test('should tokenize multi-byte string with surrogate pairs into bi-grams', () => {
    const tokens = tokenize('ğ ®·é‡å®¶ã§ğ©¸½');
    expect(tokens).toEqual([ 'ğ ®·é‡', 'é‡å®¶', 'å®¶ã§', 'ã§ğ©¸½' ]);
  });

  test('should tokenize surrogate pairs string into bi-grams', () => {
    const tokens = tokenize('ğ ®·ğ©¸½ğ ®Ÿğ €‹');
    expect(tokens).toEqual([ 'ğ ®·ğ©¸½', 'ğ©¸½ğ ®Ÿ', 'ğ ®Ÿğ €‹' ]);
  });

  test('should tokenize multi-byte string with emoji into bi-grams', () => {
    const tokens = tokenize('ã«ã‚ƒãƒ¼ã‚“ğŸ˜‡');
    expect(tokens).toEqual([ 'ã«ã‚ƒ', 'ã‚ƒãƒ¼', 'ãƒ¼ã‚“', 'ã‚“ğŸ˜‡' ]);
  });
});
